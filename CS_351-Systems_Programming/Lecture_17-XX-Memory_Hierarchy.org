#+TITLE: Lecture 17-XX
#+AUTHOR: Karl Hallsby
#+DATE: November 11, 2020 - Month DD, YYYY

* Recall
  * Von Neumann Architecture
  * Stored program computer with programs and data stored /in the same memory/.
  * We /idealize/ memory as a storage device
    - Holds programs (Instructions)
    - Holds data (Operands)
  * Memory is byte-addressible array of bytes
  * Code & data reside on the hard drive
    - ~execlp("/bin/echo", "/bin/echo", "hello", NULL)~
  * In reality, memory is a combination fo storage systems with very different characteristics
    - Registers
    - Caches
      + L1
      + L2
      + L3
    - Memory
    - Disk

* Types of Memory
** SRAM
   * Static Random Access Memory
   * Data is stable as long as power is applied
   * 6+ transistors per bit
     - D-flip-flop
     - This is complex and expensive, but very fast
   * Used for processor caches

** DRAM
   * Dynamic Random Access Memory
   * 1 Capacitor + 1 Transistor for each bit
     - Requires a periodic refresh of the bit in memory
     - *MUCH* denser and cheaper than [[*SRAM][SRAM]]
   * Main memory

** NVRAM
   * Non-Volatile Random Access Memory
   * Data persists without power
   * 1+ bits/transistor (Low read/write granularity)
   * Updates may require block erasure
   * Limited number of writes per block (100K+)
     - These disks also have wear-leveling to prevent this on a hardware level
     - Modern operating systems are aware of this fact and handle it
   * Flash disks
   * Interfaces
     - SATA
     - SCSI
     - eSATA
     - nVME (Attached to PCIe bus)

** HDD
   * Hard Disk Drive
   * Spinning magnetic platters with multiple read/write heads
     - Data access requires /mechanical seek/
     - The platters must spin to find the data.
   * Spin speeds
     - 5400 RPM
     - 7200 RPM
     - 15000 RPM
   * Random access is *very* slow
   * Sequential access is much faster, because we don't need to spin the disk to find anything
   * Price per gigabyte is *MUCH* lower
   * Much greater capacities
     - Several terabytes in a single disk.

** Distance and Speed
   * The speed of light is roughly 1 ft/ns
   * In a 3GHz CPU, we can travel 4 in/cycle
   * Thus, the maximum access distance (round trip) for access in a single cycle is 2 in.
     - Anything further will require multiple cycles to access
   * Thus, it is best to keep things as close as possible to the CPU.

** Relative Speeds
| Type                         | Size         | Access Latency     | Unit     |
|------------------------------+--------------+--------------------+----------|
| Registers                    | 8-32 words   | 0-1 cycles         | ns       |
| On-board SRAM (L1/L2 Cache)  | 32-256 KiB   | 1-3 cycles         | ns       |
| Off-board SRAM (L2/L3 Cache) | 256KiB-16MiB | ~10 cycles         | ns       |
| DRAM                         | 128MiB-64GiB | ~100 cycles        | ns       |
| SSD                          | \leq 1TiB    | ~10,000 cycles     | \micro s |
| HDD                          | \leq 4TiB    | ~10,000,000 cycles | ms       |

** Costs
| Device      | Price ($/TiB) |
|-------------+---------------|
| SATA HDD    |            38 |
| Flash USB   |            35 |
| SATA SSD    |           100 |
| NVMe SSD    |           130 |
| Optane      |          1103 |
| DDR4 Memory |          7968 |

** Memory Requirements
   * A lot of memory
   * Fast access to memory
   * Don't want to spend much money

* Memory Hierarchy
  Starting at high speeds, low capacities
  1. CPU
  2. CPU Registers
  3. SRAM CPU Caches
  4. DRAM Main memory
  5. Storage media (SSD/HDD)
  6. Remote Storage

** Main Idea
   * Use the /fast, but scarce/ memory as much as possible.
   * Fall back on the /slow but plentiful/ kind when necessary.

** Caching
   * Cache (Verb): Store away for future use.
   * Cache (Noun): For computing, an auxiliary memory from which higher-speed retrieval is possible.
   * There are a variety of algorithms to determine which data is evicted from the cache
   * Caching and replication are /VERY/ different.
   * Adding caches should /not/ affect the performance of the program
   * /What/ data do we want to cache?
   * Where do we store cached data?
     - How do we /map/ the address k to a cache slot?
     - Remember that SRAM \ll DRAM
     - You can't fit everything in memory in the cache

*** How it Works
**** SRAM cache starts empty
     1. CPU requests memory data at a memory address, k
     2. The ~fetch~ instruction fetches the data from DRAM (or lower storage media)
     3. /Cache/ data in the SRAM for later use

**** SRAM Cache is full
     1. CPU Requests data at memory address, k
     2. Check SRAM for cached data first
	* If it is present, then a cache hit occurs, return directly
     3. If it is *NOT* present, then a cache miss occurs, request from memory like normal

*** Localities of Reference
    * We need to maximize the performance of caching of memory
    * There are 2 main schools of thought:

**** Temporal locality
     * Based on how closely related the pieces of data related in time.
     * If a datum was accessed recently, it's likely to be accessed again soon.
     * For example, accessing a loop counter
     * For example, calling a function repeatedly (recursion)

**** Spatial Locality
     * Based on how closely related the pieces of data are related in memory space.
     * After accessing data at a given address, data nearby is likely to be accessed
     * If a datum was accessed, grab the surrounding data too.
     * For example, the contents of a loop
     * For example, accessing elements on an array

* Lab 5
  * Need to be sure that the loop counter can deal with the possibility fo overflowing
  * Probably should use ~unsigned

2 Ways we will measure performance
  1. Arithmetic Operations (~flops~)
     - Will likely have little impact due to memory hierarchy
     - Arithmetic operations per second
     - Will need to disassemble the for-loop to figure out how many instructions we are executing
     - *You* can't make the loop faster:
       + The compiler can optimize these, usually through loop unrolling.
	 * You "unroll" the loop so that you perform multiple iterations before looping
       + Manually "unroll" so that you have multiple arithmetic operations before looping
	 * Instruction-level parallelism
	 * Certain instructions can run concurrently without programmer input
	 * For example, you fetch new code while executing the current instruction
     - 2 types of data
       1. Single precision (4 byte ~int~ data types)
       2. Double precision (8-byte ~double~ data types)
     - Size:
       1. ~small~: 10 billion (10^10) operations
       2. ~medium~: 100 billion (10^11) operations
       3. ~large~: 1000 billion (10^12) operations
     - Threads:
       1. 1 worker thread
       2. 2 worker threads
	  * Each thread has half of the operations
       3. 4 worker threads
	  * Each thread has a quarter of the operations
  2. Matrix Multiplication (~matrix~)
     - Memory hierarchy will have a massive impact on this.
     - WILL need to ~malloc~ or ~calloc~
       + ~calloc~ has the problem of initializing memory before returning it
     - Implement matrix multiplication
       + Naive implementation is pretty simple (~10 LoC)
       + 3 nested ~for~-loops
       + Easy and straightforward
       + Time complexity is O(n^3)
     - a = b + c*d
       + Fuse-Multiply-Add
       + More efficient than the 2 pure mathematical operations
     - Size:
       1. ~small~: 1024x1024 matrices
       2. ~medium~: 4096x4096 matrices
       3. ~large~: 16384x16384 matrices
     - Threads:
       1. 1 worker thread
       2. 2 worker threads
       3. 4 worker threads
       4. Synchronization is needed for the output matrix
	  * Although, this is pretty simple to get working
#+BEGIN_SRC c
void multiply(double** mat1, double** mat2, double** res, int size) {
	int i, j, k;
	for(i = 0; i < size; i++) {
		for(j = 0; j < size; j++) {
			res[i][j] = 0;
			for(k = 0; k < size; k++) {
				res[i][j] = mat1[i][k] * mat2[k][j];
			}
		}
	}
}
#+END_SRC

** Instruction-Level Parallelism
  * Optimizations can be performed by making a later instruction depends on 2 previous ones
  * The compiler can run these 2 instructions in parallel
#+BEGIN_SRC c
e = a+b;
f = c+d;
m = e*f;
#+END_SRC

** Fused Multiply-Add
   * a \leftarrow a + (b \times c)
   * This multi-step arithmetic expression can be performed in 1 instruction

** Other Optimizations
   * GCC compiler optimizations
   * Matrix Transpose
   * Find out the size of cache lines
   * Find the size of the L1 Cache
   * Multithreading without any locks on matrices using static work partitioning
   * Give compiler hints to assign certain variables to registers
     - In C, this is done with the ~register~ keyword.
   * Explore different instructions for reading from the memory to do something

* PThreads
  * Requires the ~-lpthread~ link flag for compilation
  * Requires ~#include <pthread.h>~

* Lab Extra Credit 2 - Benchmark Thread Synchronization Primitives
  * Mode
    1. No synchronization
    2. Mutex
    3. Semaphore
    4. Spinlock
    5. Atomic
  * Size
    1. Small (100 million operations) 10^{8}
    2. Large (1 billion operations) 10^{9}
  * Threads
    1. 1 Thread
    2. 2 Threads
    3. 4 Threads
  * Metrics
    1. Throughput
       - Measure the rate at which the counter can be incremented per second
       - Use *ONLY* the large size
       - Throughput is calculated for each thread individually
       - All results should be aggregated to get the final throughput value
    2. Latency
       - Measure each individual increment operation and store it in memory
       - Use *ONLY* the small size
    3. Timing
       1. Use number of CPU cycles
       2. Need to get nanosecond resolution on these operations

* Lab Extra Credit 3 - Moore's Law Essay
  * Use the report in [[*Lab Extra Credit 2 - Benchmark Thread Synchronization Primitives][Lab Extra Credit 2 - Benchmark Thread Synchronization Primitives]] as an example

* Lab Extra Credit 4 - ~memcpy~ and ~memset~
  * These allow us to initialize or move a piece of memory around
  * Sizes
    1. Small, 4KiB
    2. Medium, 4MiB
    3. Large, 4GiB
  * Implement your own ~my_memcpy~ and ~my_memset~ to try to improve the performance of these
    - Read into SSE and AVX instructions
    - Look into different compilers
      + GCC
      + LLVM
      + ICC
    - Explore optimization flags
    - Parallelize these functions using PThreads
    - Use Valgrind to profile the code
    - If our version doesn't improve on the normal versions, won't get full points.
  * Measure throughput in bytes/second of each operation
    - Might need to perform multiple ~memset~ or ~memcpy~ operations to get accurate timing
    - An operation on just a single buffer will likely require cycle-accurate timing
